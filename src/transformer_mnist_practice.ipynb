{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with MNIST (or Fashion MNIST)\n",
    "\n",
    "* `Attention is All You Need`, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "  * Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "  \n",
    "* This code is available to tensorflow version 2.0\n",
    "* Implemented by [`tf.keras.layers`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers) [`tf.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses)\n",
    "  \n",
    "![title](pics/transformer_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'transformer_mnist'\n",
    "train_dir = os.path.join('train', model_name, 'exp')\n",
    "dataset_name = 'mnist'\n",
    "assert dataset_name in ['mnist', 'fashion_mnist']\n",
    "\n",
    "max_epochs = 50\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "width, height = 14, 14\n",
    "TEST = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "if dataset_name == 'mnist':\n",
    "  (train_images, train_labels), _ = \\\n",
    "      tf.keras.datasets.mnist.load_data()\n",
    "else:\n",
    "  (train_images, train_labels), _ = \\\n",
    "      tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "train_images = tf.image.resize(train_images, size=[width, height])\n",
    "train_images = tf.cast(train_images[:, :, :, 0], tf.int64)\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "plt.figure(figsize=[18, 4])\n",
    "for i in range(3):\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title('train image')\n",
    "    plt.imshow(train_images[i], cmap='binary')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.random.set_seed(117)\n",
    "# for train\n",
    "N = len(train_images)\n",
    "        \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference : \n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/functional.html\n",
    "# https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "\n",
    "class MultiheadAttention(tf.keras.Model):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_proj = layers.Dense(embed_dim)\n",
    "        self.k_proj = layers.Dense(embed_dim)\n",
    "        self.v_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "        self.out_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, query, key, value, attn_mask=None):\n",
    "        \n",
    "        # query : [batch, tgt_len, embed_dim]\n",
    "        # key   : [batch, src_len, embed_dim]\n",
    "        # value : [batch, src_len, embed_dim]\n",
    "        \n",
    "        batch, tgt_len, _ = query.shape\n",
    "        _, src_len, _ = key.shape\n",
    "        \n",
    "        '''\n",
    "        Query, Key, Value Projection\n",
    "        '''\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        q = self.q_proj(query)\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        k = self.k_proj(key)\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        '''\n",
    "        Reshape for Multi-Head Attention\n",
    "        '''\n",
    "        # [batch, tgt_len, num_heads, head_dim]\n",
    "        q = tf.reshape(q, [batch, tgt_len, self.num_heads, self.head_dim])\n",
    "        # [batch, num_heads, tgt_len, head_dim]\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "        \n",
    "        # [batch, src_len, num_heads, head_dim]\n",
    "        k = tf.reshape(k, [batch, src_len, self.num_heads, self.head_dim])\n",
    "        # [batch, num_heads, src_len, head_dim]\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        \n",
    "        # [batch, src_len, num_heads, head_dim]\n",
    "        v = tf.reshape(v, [batch, src_len, self.num_heads, self.head_dim])\n",
    "        # [batch, num_heads, src_len, head_dim]\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        w : Attention Output Weights\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 시작\n",
    "        '''\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/linalg/matmul?version=stable\n",
    "        # tf.linalg.matmul을 이용하여 q와 k.T (transposed)의 matrix multiplication을 계산\n",
    "        \n",
    "        # [batch, num_heads, tgt_len, src_len]\n",
    "        w = #빈칸#\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 끝\n",
    "        '''\n",
    "        \n",
    "        # scaling by square-root of head_dim\n",
    "        w = w / np.sqrt(self.head_dim)\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            w = w + attn_mask * -1e9\n",
    "            \n",
    "        # [batch, num_heads, tgt_len, src_len] \n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        \n",
    "        '''\n",
    "        a : Attention Output\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 시작\n",
    "        '''\n",
    "        \n",
    "        # tf.linalg.matmul을 이용하여 w와 v의 matrix multiplication을 계산\n",
    "        \n",
    "        # [batch, num_heads, tgt_len, head_dim]\n",
    "        a = #빈칸#\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 끝\n",
    "        '''\n",
    "        \n",
    "        # [batch, tgt_len, num_heads, head_dim]\n",
    "        a = tf.transpose(a, [0, 2, 1, 3])\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        a = tf.reshape(a, [batch, tgt_len, self.embed_dim])\n",
    "        \n",
    "        out = self.out_proj(a)\n",
    "        \n",
    "        return out, tf.reduce_mean(w, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(tf.keras.Model):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout, activation=tf.nn.relu):\n",
    "        super(Feedforward, self).__init__()\n",
    "        \n",
    "        self.linear1 = layers.Dense(dim_feedforward, activation=activation)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.linear2 = layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x, training=True)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Reference :     \n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer\n",
    "class TransformerEncoderLayer(tf.keras.Model):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=tf.nn.relu):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.feed_forward = Feedforward(d_model, dim_feedforward, dropout, activation)\n",
    "        \n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def call(self, src, src_mask=None):\n",
    "        # src : [batch, length, d_model]\n",
    "        \n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2, training=True)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + self.dropout2(src2, training=True)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "    \n",
    "# Reference : \n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoder\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation) \\\n",
    "                       for _ in range(num_layers)]\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def call(self, src, mask=None):\n",
    "        # src : [batch, src_len, embed_dim]\n",
    "        \n",
    "        output = src\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.encoder_layers[i](output, src_mask=mask)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, d_model=512, vocab_size=256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(d_model=d_model, nhead=8, dim_feedforward=2048, dropout=0.1,\n",
    "                                                     activation=tf.nn.relu, num_layers=6)\n",
    "        self.logit_layer = layers.Dense(vocab_size)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    @staticmethod\n",
    "    def _positional_encoding(position, d_model):\n",
    "        angle_rads = Model._get_angles(np.arange(position)[:, np.newaxis],\n",
    "                               np.arange(d_model)[np.newaxis, :],\n",
    "                               d_model)\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_look_ahead_mask(size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def call(self, images):\n",
    "        # images : [batch, src_length]\n",
    "        \n",
    "        _, src_length = images.shape\n",
    "        \n",
    "        # [batch, src_length, d_model]\n",
    "        src = self.embedding(images) + self._positional_encoding(src_length, self.d_model)\n",
    "        \n",
    "        mask = self._create_look_ahead_mask(src_length)\n",
    "        output = self.transformer_encoder(src, mask=mask)\n",
    "        logit = self.logit_layer(output)\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    def inference(self, images):\n",
    "        from tqdm.notebook import tqdm\n",
    "\n",
    "        for i in tqdm(range(0, width*height)):\n",
    "            logit = self.call(images)\n",
    "            sample = tf.random.categorical(logit[:, i], num_samples=1)\n",
    "            images = tf.concat([images[:, :i+1], sample, images[:, i+2:]], axis=1)\n",
    "        \n",
    "        return images[:, 1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create a model object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D-image to 1D-vector, 1D-vector to 2D-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(images):\n",
    "    batch, height, width = images.shape\n",
    "    vector = tf.reshape(images, [batch, height*width])\n",
    "    return vector\n",
    "\n",
    "def vector_to_image(vectors, height, width):\n",
    "    batch, dim = vectors.shape\n",
    "    images = tf.reshape(vectors, [batch, height, width])\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a unconditional image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = model.inference(tf.zeros([1, width*height], dtype=tf.int64))\n",
    "pred = vector_to_image(images, width, height)\n",
    "plt.imshow(pred[0], cmap='binary')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define a single train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, images):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "           \n",
    "        images_shifted = tf.concat([tf.zeros([images.shape[0], 1], dtype=tf.int64), images[:, :-1]], axis=1)\n",
    "        logit = model(images_shifted)\n",
    "        loss = cce(images, logit)\n",
    "        \n",
    "    gradient = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "    \n",
    "    return logit, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Main Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for i, images in enumerate(train_dataset):\n",
    "        images = image_to_vector(images)\n",
    "        \n",
    "        logit, loss = train_step(model, optimizer, images)\n",
    "        model.global_step.assign_add(1)\n",
    "        global_step = model.global_step.numpy()\n",
    "        \n",
    "        if global_step % 100 == 0:\n",
    "            print('global step :', global_step, 'cross-entropy loss :', loss.numpy())\n",
    "        \n",
    "        if global_step % 1000 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "            print('Creating Images...')\n",
    "            images = model.inference(tf.zeros([10, width*height], dtype=tf.int64))\n",
    "            pred = vector_to_image(images, width, height)\n",
    "            \n",
    "            plt.figure(figsize=[18, 3])\n",
    "            for j in range(10):\n",
    "                plt.subplot(1, 10, j+1)\n",
    "                plt.imshow(pred[j], cmap='binary')\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "            plt.show()     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
