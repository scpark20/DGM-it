{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICE with MNIST\n",
    "\n",
    "* `NICE: Non-Linear Independent Components Estimation`, [arXiv:1410.8516](https://arxiv.org/abs/1410.8516)\n",
    "  * Laurent Dinh, David Krueger and Yoshua Bengio\n",
    "  \n",
    "* Implemented by [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) and [`eager execution`](https://www.tensorflow.org/guide/eager)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'nice'\n",
    "train_dir = os.path.join('train', model_name, 'exp1')\n",
    "\n",
    "max_epochs = 1500\n",
    "save_model_epochs = 100\n",
    "print_steps = 50\n",
    "save_images_epochs = 50\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "num_examples_to_generate = 16\n",
    "MNIST_SIZE = 28\n",
    "noise_dim = MNIST_SIZE**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data.reshape(-1, MNIST_SIZE**2).astype('float32')\n",
    "train_data = train_data / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_uniform_noise(image):\n",
    "    noise = tf.random.uniform(image.get_shape(), maxval=1./255)\n",
    "    return tf.clip_by_value(image + noise, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.random.set_seed(219)\n",
    "\n",
    "# for train\n",
    "N = len(train_data)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.map(_add_uniform_noise)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the NICE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUMLP(tf.keras.Model):\n",
    "    def __init__(self, input_size):\n",
    "        super(ReLUMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = layers.Dense(units=1000, activation='relu')\n",
    "        self.fc2 = layers.Dense(units=1000, activation='relu')\n",
    "        self.fc3 = layers.Dense(units=1000, activation='relu')\n",
    "        self.fc4 = layers.Dense(units=MNIST_SIZE**2-self.input_size)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        fc1 = self.fc1(inputs)\n",
    "        fc2 = self.fc2(fc1)\n",
    "        fc3 = self.fc3(fc2)\n",
    "        fc4 = self.fc4(fc3)\n",
    "\n",
    "        return fc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(inputs, method='oddeven', p1_size=MNIST_SIZE**2//2):\n",
    "    if method == 'oddeven':\n",
    "        partition1 = inputs[:, 0::2]\n",
    "        partition2 = inputs[:, 1::2]\n",
    "    elif method == 'topdown':\n",
    "        partition1 = inputs[:, :p1_size]\n",
    "        partition2 = inputs[:, p1_size:]\n",
    "    else:\n",
    "        raise ValueError('Not allowed method')\n",
    "\n",
    "    return partition1, partition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(partition1, partition2, method='oddeven'):\n",
    "    if method == 'oddeven':\n",
    "        merged = []\n",
    "        for j in range(partition1.shape[1]):\n",
    "            merged.append(partition1[:,j])\n",
    "            merged.append(partition2[:,j])\n",
    "        merged = tf.stack(merged, axis=1)\n",
    "    elif method == 'topdown':\n",
    "        merged = tf.concat((partition1, partition2), axis=1)\n",
    "    else:\n",
    "        raise ValueError('Not allowed method')\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveCouplingLayer(tf.keras.Model):\n",
    "    def __init__(self, partition_method, input_size):\n",
    "        super(AdditiveCouplingLayer, self).__init__()\n",
    "        self.partition_method = partition_method\n",
    "        self.input_size = input_size\n",
    "        self.relumlp = ReLUMLP(self.input_size)\n",
    "\n",
    "        # (random) permutation index\n",
    "        indices = np.asarray(range(MNIST_SIZE**2), dtype=np.int32)\n",
    "        indices = np.random.permutation(indices)\n",
    "\n",
    "        # Reverse it\n",
    "        indices_inverse = np.zeros(shape=indices.shape, dtype=np.int32)\n",
    "        for i in range(MNIST_SIZE**2):\n",
    "            indices_inverse[indices[i]] = i\n",
    "\n",
    "        # convert to tensor\n",
    "        self.tf_indices = tf.Variable(indices, trainable=None, name='tf_indices', dtype=tf.int32)\n",
    "        self.tf_indices_inverse = tf.Variable(indices_inverse, trainable=None, name='tf_indices_inverse', dtype=tf.int32)\n",
    "\n",
    "    def call(self, x):\n",
    "        # permutation x\n",
    "        x = tf.gather(x, self.tf_indices, axis=1)\n",
    "        x1, x2 = partition(x, self.partition_method)\n",
    "        y1 = x1\n",
    "        y2 = x2 + self.relumlp(x1)\n",
    "\n",
    "        return merge(y1, y2, self.partition_method)\n",
    "\n",
    "    def inverse(self, y):\n",
    "        y1, y2 = partition(y, self.partition_method)\n",
    "        x1 = y1\n",
    "        x2 = y2 - self.relumlp(y1)\n",
    "        x = merge(x1, x2, self.partition_method)\n",
    "        # inverse permutation x\n",
    "        x = tf.gather(x, self.tf_indices_inverse, axis=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NICE(tf.keras.Model):\n",
    "    def __init__(self, partition_method, partition_size):\n",
    "        super(NICE, self).__init__()\n",
    "        self.partition_method = partition_method\n",
    "        self.partition_size1 = partition_size\n",
    "        self.partition_size2 = MNIST_SIZE**2 - partition_size\n",
    "\n",
    "        self.coupling1 = AdditiveCouplingLayer(self.partition_method, self.partition_size1)\n",
    "        self.coupling2 = AdditiveCouplingLayer(self.partition_method, self.partition_size2)\n",
    "        self.coupling3 = AdditiveCouplingLayer(self.partition_method, self.partition_size1)\n",
    "        self.coupling4 = AdditiveCouplingLayer(self.partition_method, self.partition_size2)\n",
    "        self.log_scaling = tf.Variable(np.zeros([MNIST_SIZE**2]), trainable=True, name='log_scaling', dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h1 = self.coupling1(inputs)\n",
    "        h2 = self.coupling2(h1)\n",
    "        h3 = self.coupling3(h2)\n",
    "        h4 = self.coupling4(h3)\n",
    "        h = h4 * tf.exp(self.log_scaling)\n",
    "\n",
    "        return h, self.log_scaling\n",
    "\n",
    "    def generate_sample(self, noise_vector):\n",
    "        h4 = noise_vector / tf.exp(self.log_scaling)\n",
    "        h3 = self.coupling4.inverse(h4)\n",
    "        h2 = self.coupling3.inverse(h3)\n",
    "        h1 = self.coupling2.inverse(h2)\n",
    "        x = self.coupling1.inverse(h1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice = NICE(partition_method='oddeven', partition_size=MNIST_SIZE**2//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the loss functions and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(h, prior='gaussian'):\n",
    "    if prior == 'logistic':\n",
    "        log_likelihood = -tf.reduce_sum( tf.math.softplus(h) + tf.math.softplus(-h), axis=1 )\n",
    "    elif prior == 'gaussian':\n",
    "        log_likelihood = -0.5 * tf.reduce_sum(h**2, axis=1)\n",
    "\n",
    "    return -tf.reduce_mean(log_likelihood, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement of the nice.\n",
    "location = 0.0 # location\n",
    "scale = 0.5 # scale\n",
    "random_vector_for_generation = tf.random.normal([num_examples_to_generate, noise_dim], mean=location, stddev=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training one step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hidden_state, log_scaling = nice(images)\n",
    "        nll = negative_log_likelihood(hidden_state, prior='logistic')\n",
    "        ss = -tf.reduce_sum(log_scaling) # sum of log scaling\n",
    "        loss = nll + ss\n",
    "\n",
    "    gradients = tape.gradient(loss, nice.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, nice.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Start Training.')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for step, images in enumerate(train_dataset):\n",
    "\n",
    "        loss = train_step(images)\n",
    "        \n",
    "        if global_step % 10 == 0:\n",
    "            print('step :', global_step, 'loss :', loss.numpy())\n",
    "        \n",
    "        if global_step % 100 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            sample_images = nice.generate_sample(random_vector_for_generation)\n",
    "            sample_images = tf.clip_by_value(sample_images, 0, 1)\n",
    "            sample_images = tf.reshape(sample_images, [sample_images.shape[0], 28, 28])\n",
    "            \n",
    "            plt.figure(figsize=[18, 3])\n",
    "            for j in range(10):\n",
    "                plt.subplot(1, 10, j+1)\n",
    "                plt.imshow(sample_images[j], cmap='binary')\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "            plt.show()  \n",
    "            \n",
    "        global_step += 1\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
