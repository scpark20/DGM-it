{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with MNIST (or Fashion MNIST)\n",
    "\n",
    "* `Attention is All You Need`, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "  * Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "  \n",
    "* This code is available to tensorflow version 2.0\n",
    "* Implemented by [`tf.keras.layers`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers) [`tf.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'transformer_mnist'\n",
    "train_dir = os.path.join('train', model_name, 'exp')\n",
    "dataset_name = 'mnist'\n",
    "assert dataset_name in ['mnist', 'fashion_mnist']\n",
    "\n",
    "max_epochs = 50\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "width, height = 14, 14\n",
    "TEST = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 14, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "if dataset_name == 'mnist':\n",
    "  (train_images, train_labels), _ = \\\n",
    "      tf.keras.datasets.mnist.load_data()\n",
    "else:\n",
    "  (train_images, train_labels), _ = \\\n",
    "      tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "train_images = tf.image.resize(train_images, size=[width, height])\n",
    "train_images = tf.cast(train_images[:, :, :, 0], tf.int64)\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "plt.figure(figsize=[18, 4])\n",
    "for i in range(3):\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title('train image')\n",
    "    plt.imshow(train_images[i], cmap='binary')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 14, 14), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "#tf.random.set_seed(117)\n",
    "# for train\n",
    "N = len(train_images)\n",
    "        \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference : \n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/functional.html\n",
    "# https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "\n",
    "class MultiheadAttention(tf.keras.Model):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_proj = layers.Dense(embed_dim)\n",
    "        self.k_proj = layers.Dense(embed_dim)\n",
    "        self.v_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "        self.out_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, query, key, value, attn_mask=None):\n",
    "        \n",
    "        # query : [batch, tgt_len, embed_dim]\n",
    "        # key   : [batch, src_len, embed_dim]\n",
    "        # value : [batch, src_len, embed_dim]\n",
    "        \n",
    "        batch, tgt_len, _ = query.shape\n",
    "        _, src_len, _ = key.shape\n",
    "        \n",
    "        '''\n",
    "        Query, Key, Value Projection\n",
    "        '''\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        q = self.q_proj(query)\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        k = self.k_proj(key)\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        '''\n",
    "        Reshape for Multi-Head Attention\n",
    "        '''\n",
    "        # [batch, tgt_len, num_heads, head_dim]\n",
    "        q = tf.reshape(q, [batch, tgt_len, self.num_heads, self.head_dim])\n",
    "        # [batch, num_heads, tgt_len, head_dim]\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "        \n",
    "        # [batch, src_len, num_heads, head_dim]\n",
    "        k = tf.reshape(k, [batch, src_len, self.num_heads, self.head_dim])\n",
    "        # [batch, num_heads, src_len, head_dim]\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        \n",
    "        # [batch, src_len, num_heads, head_dim]\n",
    "        v = tf.reshape(v, [batch, src_len, self.num_heads, self.head_dim])\n",
    "        # [batch, num_heads, src_len, head_dim]\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        w : Attention Output Weights\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 시작\n",
    "        '''\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/linalg/matmul?version=stable\n",
    "        # tf.linalg.matmul을 이용하여 q와 k.T (transposed)의 matrix multiplication을 계산\n",
    "        \n",
    "        # [batch, num_heads, tgt_len, src_len]\n",
    "        w = #빈칸#\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 끝\n",
    "        '''\n",
    "        \n",
    "        # scaling by square-root of head_dim\n",
    "        w = w / np.sqrt(self.head_dim)\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            w = w + attn_mask * -1e9\n",
    "            \n",
    "        # [batch, num_heads, tgt_len, src_len] \n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        \n",
    "        '''\n",
    "        a : Attention Output\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 시작\n",
    "        '''\n",
    "        \n",
    "        # tf.linalg.matmul을 이용하여 w와 v의 matrix multiplication을 계산\n",
    "        \n",
    "        # [batch, num_heads, tgt_len, head_dim]\n",
    "        a = #빈칸#\n",
    "        \n",
    "        '''\n",
    "        빈칸 채우기 끝\n",
    "        '''\n",
    "        \n",
    "        # [batch, tgt_len, num_heads, head_dim]\n",
    "        a = tf.transpose(a, [0, 2, 1, 3])\n",
    "        # [batch, tgt_len, embed_dim]\n",
    "        a = tf.reshape(a, [batch, tgt_len, self.embed_dim])\n",
    "        \n",
    "        out = self.out_proj(a)\n",
    "        \n",
    "        return out, tf.reduce_mean(w, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(tf.keras.Model):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout, activation=tf.nn.relu):\n",
    "        super(Feedforward, self).__init__()\n",
    "        \n",
    "        self.linear1 = layers.Dense(dim_feedforward, activation=activation)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.linear2 = layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x, training=True)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Reference :     \n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer\n",
    "class TransformerEncoderLayer(tf.keras.Model):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=tf.nn.relu):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.feed_forward = Feedforward(d_model, dim_feedforward, dropout, activation)\n",
    "        \n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def call(self, src, src_mask=None):\n",
    "        # src : [batch, length, d_model]\n",
    "        \n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "        src = src + self.dropout1(src2, training=True)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + self.dropout2(src2, training=True)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "    \n",
    "# Reference : \n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoder\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation) \\\n",
    "                       for _ in range(num_layers)]\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def call(self, src, mask=None):\n",
    "        # src : [batch, src_len, embed_dim]\n",
    "        \n",
    "        output = src\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.encoder_layers[i](output, src_mask=mask)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, d_model=512, vocab_size=256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(d_model=d_model, nhead=8, dim_feedforward=2048, dropout=0.1,\n",
    "                                                     activation=tf.nn.relu, num_layers=6)\n",
    "        self.logit_layer = layers.Dense(vocab_size)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    @staticmethod\n",
    "    def _positional_encoding(position, d_model):\n",
    "        angle_rads = Model._get_angles(np.arange(position)[:, np.newaxis],\n",
    "                               np.arange(d_model)[np.newaxis, :],\n",
    "                               d_model)\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_look_ahead_mask(size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def call(self, images):\n",
    "        # images : [batch, src_length]\n",
    "        \n",
    "        _, src_length = images.shape\n",
    "        \n",
    "        # [batch, src_length, d_model]\n",
    "        src = self.embedding(images) + self._positional_encoding(src_length, self.d_model)\n",
    "        \n",
    "        mask = self._create_look_ahead_mask(src_length)\n",
    "        output = self.transformer_encoder(src, mask=mask)\n",
    "        logit = self.logit_layer(output)\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    def inference(self, images):\n",
    "        from tqdm.notebook import tqdm\n",
    "\n",
    "        for i in tqdm(range(0, width*height)):\n",
    "            logit = self.call(images)\n",
    "            sample = tf.random.categorical(logit[:, i], num_samples=1)\n",
    "            images = tf.concat([images[:, :i+1], sample, images[:, i+2:]], axis=1)\n",
    "        \n",
    "        return images[:, 1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create a model object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D-image to 1D-vector, 1D-vector to 2D-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(images):\n",
    "    batch, height, width = images.shape\n",
    "    vector = tf.reshape(images, [batch, height*width])\n",
    "    return vector\n",
    "\n",
    "def vector_to_image(vectors, height, width):\n",
    "    batch, dim = vectors.shape\n",
    "    images = tf.reshape(vectors, [batch, height, width])\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a unconditional image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = model.inference(tf.zeros([1, width*height], dtype=tf.int64))\n",
    "pred = vector_to_image(images, width, height)\n",
    "plt.imshow(pred[0], cmap='binary')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define a single train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, images):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "           \n",
    "        images_shifted = tf.concat([tf.zeros([images.shape[0], 1], dtype=tf.int64), images[:, :-1]], axis=1)\n",
    "        logit = model(images_shifted)\n",
    "        loss = cce(images, logit)\n",
    "        \n",
    "    gradient = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "    \n",
    "    return logit, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Main Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for i, images in enumerate(train_dataset):\n",
    "        images = image_to_vector(images)\n",
    "        \n",
    "        logit, loss = train_step(model, optimizer, images)\n",
    "        model.global_step.assign_add(1)\n",
    "        global_step = model.global_step.numpy()\n",
    "        \n",
    "        if global_step % 100 == 0:\n",
    "            print('global step :', global_step, 'cross-entropy loss :', loss.numpy())\n",
    "        \n",
    "        if global_step % 1000 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "            print('Creating Images...')\n",
    "            images = model.inference(tf.zeros([10, width*height], dtype=tf.int64))\n",
    "            pred = vector_to_image(images, width, height)\n",
    "            \n",
    "            plt.figure(figsize=[18, 3])\n",
    "            for j in range(10):\n",
    "                plt.subplot(1, 10, j+1)\n",
    "                plt.imshow(pred[j], cmap='binary')\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "            plt.show()     \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
