{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation with Categorical Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "max_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a toy dataset (categorical distribution)\n",
    "\n",
    "**categorical distribution**\n",
    "\n",
    "$X$ is a random variable\n",
    "$$\\Pr(X=i|\\mathbf{p})=p_i, where \\; \\mathbf{p}=(p_1,\\cdots,p_k)$$\n",
    "\n",
    "**Probability mass function**\n",
    "$$f(\\mathbf{x};\\mathbf{p})=\\prod_{i=1}^{k} p_i^{x_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 3 ... 3 3 3]\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "N = 10000 # the number of samples\n",
    "C = 5     # the number of categories\n",
    "true_probs = [0.1, 0.2, 0.1, 0.4, 0.2]\n",
    "train_data = np.random.choice(C, size=N, p=true_probs)\n",
    "print(train_data)\n",
    "train_data = train_data.astype(np.float32)\n",
    "train_data = np.expand_dims(train_data, axis=1)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (128, 1), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "# for train\n",
    "N = len(train_data)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the parameters to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log-likelihood in Bernoulli distribution**\n",
    "$$\\log f(\\mathbf{x}|\\mathbf{p})=\\sum_{i=1}^{k}\\log p_i^{x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "\n",
    "* `logit`: The probabilities can be obtained by applying softmax to the logit values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>\n",
      "tf.Tensor([0.2 0.2 0.2 0.2 0.2], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "logit = tf.Variable(np.array([0., 0., 0., 0., 0.]), dtype=tf.float32) # initial value\n",
    "print(logit)\n",
    "\n",
    "probs = tf.nn.softmax(logit)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pmf(sample, probs):\n",
    "    log_likelihood = None\n",
    "    for i in range(C):\n",
    "        if log_likelihood is None:\n",
    "            log_likelihood = tf.cast((data == i), tf.float32) * tf.math.log(probs[i])\n",
    "        else:\n",
    "            log_likelihood = log_likelihood + tf.cast((data == i), tf.float32) * tf.math.log(probs[i])\n",
    "\n",
    "    return log_likelihood\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss functions and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training one step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        negative_log_likelihood = -tf.reduce_mean(log_pmf(data, tf.nn.softmax(logit)))\n",
    "\n",
    "    gradients = tape.gradient(negative_log_likelihood, [logit])\n",
    "    optimizer.apply_gradients(zip(gradients, [logit]))\n",
    "\n",
    "    return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training full steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training.\n",
      "epoch : 0 NLL : 1.5092432\n",
      "epoch : 1 NLL : 1.5075796\n",
      "epoch : 2 NLL : 1.50599\n",
      "epoch : 3 NLL : 1.504471\n",
      "epoch : 4 NLL : 1.5030192\n",
      "epoch : 5 NLL : 1.501632\n",
      "epoch : 6 NLL : 1.5003058\n",
      "epoch : 7 NLL : 1.4990379\n",
      "epoch : 8 NLL : 1.4978256\n",
      "epoch : 9 NLL : 1.4966663\n",
      "epoch : 10 NLL : 1.4955578\n",
      "epoch : 11 NLL : 1.4944972\n",
      "epoch : 12 NLL : 1.4934825\n",
      "epoch : 13 NLL : 1.4925119\n",
      "epoch : 14 NLL : 1.4915829\n",
      "epoch : 15 NLL : 1.4906938\n",
      "epoch : 16 NLL : 1.4898427\n",
      "epoch : 17 NLL : 1.4890277\n",
      "epoch : 18 NLL : 1.4882476\n",
      "epoch : 19 NLL : 1.4875004\n",
      "epoch : 20 NLL : 1.4867848\n",
      "epoch : 21 NLL : 1.486099\n",
      "epoch : 22 NLL : 1.4854424\n",
      "epoch : 23 NLL : 1.484813\n",
      "epoch : 24 NLL : 1.4842098\n",
      "epoch : 25 NLL : 1.4836316\n",
      "epoch : 26 NLL : 1.4830773\n",
      "epoch : 27 NLL : 1.4825456\n",
      "epoch : 28 NLL : 1.4820359\n",
      "epoch : 29 NLL : 1.481547\n",
      "epoch : 30 NLL : 1.4810779\n",
      "epoch : 31 NLL : 1.4806279\n",
      "epoch : 32 NLL : 1.480196\n",
      "epoch : 33 NLL : 1.4797815\n",
      "epoch : 34 NLL : 1.4793835\n",
      "epoch : 35 NLL : 1.4790016\n",
      "epoch : 36 NLL : 1.4786346\n",
      "epoch : 37 NLL : 1.4782823\n",
      "epoch : 38 NLL : 1.477944\n",
      "epoch : 39 NLL : 1.4776187\n",
      "epoch : 40 NLL : 1.4773064\n",
      "epoch : 41 NLL : 1.477006\n",
      "epoch : 42 NLL : 1.4767175\n",
      "epoch : 43 NLL : 1.4764398\n",
      "epoch : 44 NLL : 1.4761732\n",
      "epoch : 45 NLL : 1.4759165\n",
      "epoch : 46 NLL : 1.4756696\n",
      "epoch : 47 NLL : 1.4754323\n",
      "epoch : 48 NLL : 1.4752035\n",
      "epoch : 49 NLL : 1.4749838\n",
      "epoch : 50 NLL : 1.4747722\n",
      "epoch : 51 NLL : 1.4745686\n",
      "epoch : 52 NLL : 1.4743721\n",
      "epoch : 53 NLL : 1.4741833\n",
      "epoch : 54 NLL : 1.4740014\n",
      "epoch : 55 NLL : 1.4738262\n",
      "epoch : 56 NLL : 1.4736574\n",
      "epoch : 57 NLL : 1.4734948\n",
      "epoch : 58 NLL : 1.4733378\n",
      "epoch : 59 NLL : 1.4731867\n",
      "epoch : 60 NLL : 1.4730409\n",
      "epoch : 61 NLL : 1.4729004\n",
      "epoch : 62 NLL : 1.472765\n",
      "epoch : 63 NLL : 1.4726341\n",
      "epoch : 64 NLL : 1.472508\n",
      "epoch : 65 NLL : 1.4723864\n",
      "epoch : 66 NLL : 1.472269\n",
      "epoch : 67 NLL : 1.4721557\n",
      "epoch : 68 NLL : 1.4720463\n",
      "epoch : 69 NLL : 1.4719404\n",
      "epoch : 70 NLL : 1.4718387\n",
      "epoch : 71 NLL : 1.47174\n",
      "epoch : 72 NLL : 1.4716449\n",
      "epoch : 73 NLL : 1.4715528\n",
      "epoch : 74 NLL : 1.4714642\n",
      "epoch : 75 NLL : 1.4713783\n",
      "epoch : 76 NLL : 1.4712952\n",
      "epoch : 77 NLL : 1.4712152\n",
      "epoch : 78 NLL : 1.4711378\n",
      "epoch : 79 NLL : 1.4710627\n",
      "epoch : 80 NLL : 1.4709902\n",
      "epoch : 81 NLL : 1.47092\n",
      "epoch : 82 NLL : 1.4708521\n",
      "epoch : 83 NLL : 1.4707865\n",
      "epoch : 84 NLL : 1.470723\n",
      "epoch : 85 NLL : 1.4706614\n",
      "epoch : 86 NLL : 1.4706022\n",
      "epoch : 87 NLL : 1.4705443\n",
      "epoch : 88 NLL : 1.4704885\n",
      "epoch : 89 NLL : 1.4704347\n",
      "epoch : 90 NLL : 1.4703822\n",
      "epoch : 91 NLL : 1.4703315\n",
      "epoch : 92 NLL : 1.4702826\n",
      "epoch : 93 NLL : 1.470235\n",
      "epoch : 94 NLL : 1.4701889\n",
      "epoch : 95 NLL : 1.4701443\n",
      "epoch : 96 NLL : 1.470101\n",
      "epoch : 97 NLL : 1.4700589\n",
      "epoch : 98 NLL : 1.4700181\n",
      "epoch : 99 NLL : 1.4699788\n",
      "Training Done.\n"
     ]
    }
   ],
   "source": [
    "print('Start Training.')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for step, data in enumerate(train_dataset):\n",
    "\n",
    "        negative_log_likelihood = train_step(data)\n",
    "        global_step.assign_add(1)\n",
    "\n",
    "    print('epoch :', epoch, 'NLL :', negative_log_likelihood.numpy())\n",
    "\n",
    "print('Training Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "estimated probabilities : [0.12301521 0.18276438 0.08774455 0.39012673 0.21634908]\n",
      "true probabilites : [0.1, 0.2, 0.1, 0.4, 0.2]\n"
     ]
    }
   ],
   "source": [
    "print('Results')\n",
    "print('estimated probabilities :', tf.nn.softmax(logit).numpy())\n",
    "print('true probabilites :', true_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
